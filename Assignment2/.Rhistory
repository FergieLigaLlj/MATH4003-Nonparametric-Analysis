classes = matrix(as.numeric(mod.opt), n.grid1, n.grid1)
class_colors = c("#4E79A7", "#F28E2B")
# Add some transparency to make the fill colours less bright
fill_colors = paste0(class_colors, "88")
# Use image to plot the predicted class at each point
image(x1.grid1, x2.grid1, classes, col = fill_colors,
main = "plot of training data with decision boundary k=40",
xlab = colnames(trainxx)[1], ylab = colnames(trainxx)[2])
points(trainxx, col = class_colors[trainyy], pch = 16)
### Question 2:
set.seed(456)
X1 = rnorm(n=100,0,1)
X2 = rnorm(n=100,0,1)
X3 = rnorm(n=100,0,1)
X4 = rnorm(n=100,0,1)
X5 = rnorm(n=100,0,1)
error = rnorm(n=100,0,0.5)
Y =2+3*X1+X2+2*X3+error
data = cbind(X1,X2,X3,X4,X5,Y)
sample <- sample(c(TRUE,FALSE), nrow(data),replace=TRUE, prob=c(0.7,0.3))
train_dataset  <- data[sample, ]
test_dataset  <- data[!sample, ]
### Forward Selection
### we set alpha = 0.05
alpha = 0.05
### when p = 1
Rss_list = vector(mode="list",length=0)
y=train_dataset[,6]
x=train_dataset[,1]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,1]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,2]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,2]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,3]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,3]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,4]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,4]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,5]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,5]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
Rss_list=unlist(Rss_list)
R1_1 = mean(Rss_list)
n = which.min(Rss_list)
print(paste("First Chosen One is X()",n))
### When p = 2
Rss_list = vector(mode="list",length=0)
y=train_dataset[,6]
x=train_dataset[,c(1,2)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,2)]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,c(1,3)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,3)]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,c(1,4)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,4)]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,c(1,5)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,5)]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
Rss_list=unlist(Rss_list)
n = which.min(Rss_list)
R2_2 = mean(Rss_list)
print("Second model in p=2 is chosen, so It is now X1~X3")
### When p = 3
Rss_list = vector(mode="list",length=0)
y=train_dataset[,6]
x=train_dataset[,c(1,2,3)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,2,3)]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,c(1,3,4)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,3,4)]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,c(1,3,5)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,3,5)]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
Rss_list=unlist(Rss_list)
n = which.min(Rss_list)
R3_3 = mean(Rss_list)
print("Third model in p=3 is chosen, however model 2nd and 3rd all have p value very large, so we choose first model.")
print("This is given that critical value alpha = 0.05 > p-value of 2nd and 3rd model.")
RSS = c(R1_1,R2_2,R3_3)
index = c(1,2,3)
plot(index,RSS,type = "b",pch=18)
### It can be inferred from the graph that there is a elbow in p = 3, hence there is no need to further more add variable
### And current R^2 is 0.987
### Backward Selection:
### we set alpha = 0.05
alpha = 0.05
y=train_dataset[,6]
x=train_dataset[,1:5]
fit = lm(y~x)
summary(fit)
print("We can see that X5 has largest p value, so we discard X5 now.")
y=train_dataset[,6]
x=train_dataset[,1:4]
fit = lm(y~x)
summary(fit)
print("We can see that X5 has largest p value, so we discard X4 now.")
y=train_dataset[,6]
x=train_dataset[,1:3]
fit = lm(y~x)
summary(fit)
print("Now that P-value for every variable is 2e-16<0.05=alpha now, So Y~X1+X2+X3 is the final model.")
### Best Model Selection
### p = 1
Rsquare <- function(pred,test){
SSr = sum((pred-test)^2)
SSt = sum((test-mean(test))^2)
R2 = SSr/SSt
return (R2)
}
Rz = vector(mode="list",length=0)
y=train_dataset[,6]
x=train_dataset[,1]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,1]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
y=train_dataset[,6]
x=train_dataset[,2]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,2]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
y=train_dataset[,6]
x=train_dataset[,3]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,3]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
y=train_dataset[,6]
x=train_dataset[,4]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,4]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
y=train_dataset[,6]
x=train_dataset[,5]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,5]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
### p = 2
y=train_dataset[,6]
x=train_dataset[,c(1,2)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,2)]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
View(Rz)
library(caret)
library(lattice)
### True Decision Boundary
library(ggplot2)
library(MASS)
mu1 = c(1,1)
mu1 = t(t(mu1))
mu2 = c(2,2)
mu2 = t(t(mu2))
I2 = diag(2)
set.seed(123)
X1 = mvrnorm(n=50,mu1,I2)
set.seed(123)
X2 = mvrnorm(n=50,mu2,I2)
I = t(t(rep(1,50)))
mnI=t(t(rep(-1,50)))
X1 = cbind(X1,I)
X2 = cbind(X2,mnI)
Xtotal = rbind (X1,X2)
X_first = Xtotal[,1]
X_second = Xtotal[,2]
color = c(rep("blue",50),rep("red",50))
ggplot()+geom_point(mapping=aes(x=X_first,y=X_second),color=color,size=1)+geom_abline(slope=-1,intercept = 3,color="Green")
III = c(rep("1",50),rep("-1",50))
Xtotal = cbind(Xtotal[,1:2],III)
trainxx = rbind(X1,X2)
trainxx = trainxx[,1:2]
trainyy = Xtotal[,3]
n.grid1 <- 50
x1.grid1 <- seq(f = min(trainxx[, 1]), t = max(trainxx[, 1]), l = n.grid1)
x2.grid1 <- seq(f = min(trainxx[, 2]), t = max(trainxx[, 2]), l = n.grid1)
grid <- expand.grid(x1.grid1, x2.grid1)
### K=1
library("class")
mod.opt <- knn(trainxx, grid, trainyy, k = 1, prob = T)
prob_knn <- attr(mod.opt, "prob")
# Use the predicted class at each point
classes = matrix(as.numeric(mod.opt), n.grid1, n.grid1)
class_colors = c("#4E79A7", "#F28E2B")
# Add some transparency to make the fill colours less bright
fill_colors = paste0(class_colors, "88")
# Use image to plot the predicted class at each point
image(x1.grid1, x2.grid1, classes, col = fill_colors,
main = "plot of training data with decision boundary k=1",
xlab = colnames(trainxx)[1], ylab = colnames(trainxx)[2])
points(trainxx, col = class_colors[trainyy], pch = 16)
### k=3
mod.opt <- knn(trainxx, grid, trainyy, k = 3, prob = T)
prob_knn <- attr(mod.opt, "prob")
# Use the predicted class at each point
classes = matrix(as.numeric(mod.opt), n.grid1, n.grid1)
class_colors = c("#4E79A7", "#F28E2B")
# Add some transparency to make the fill colours less bright
fill_colors = paste0(class_colors, "88")
# Use image to plot the predicted class at each point
image(x1.grid1, x2.grid1, classes, col = fill_colors,
main = "plot of training data with decision boundary k=3",
xlab = colnames(trainxx)[1], ylab = colnames(trainxx)[2])
points(trainxx, col = class_colors[trainyy], pch = 16)
### k=5
mod.opt <- knn(trainxx, grid, trainyy, k = 5, prob = T)
prob_knn <- attr(mod.opt, "prob")
# Use the predicted class at each point
classes = matrix(as.numeric(mod.opt), n.grid1, n.grid1)
class_colors = c("#4E79A7", "#F28E2B")
# Add some transparency to make the fill colours less bright
fill_colors = paste0(class_colors, "88")
# Use image to plot the predicted class at each point
image(x1.grid1, x2.grid1, classes, col = fill_colors,
main = "plot of training data with decision boundary k=5",
xlab = colnames(trainxx)[1], ylab = colnames(trainxx)[2])
points(trainxx, col = class_colors[trainyy], pch = 16)
### k = 40
mod.opt <- knn(trainxx, grid, trainyy, k = 40, prob = T)
prob_knn <- attr(mod.opt, "prob")
# Use the predicted class at each point
classes = matrix(as.numeric(mod.opt), n.grid1, n.grid1)
class_colors = c("#4E79A7", "#F28E2B")
# Add some transparency to make the fill colours less bright
fill_colors = paste0(class_colors, "88")
# Use image to plot the predicted class at each point
image(x1.grid1, x2.grid1, classes, col = fill_colors,
main = "plot of training data with decision boundary k=40",
xlab = colnames(trainxx)[1], ylab = colnames(trainxx)[2])
points(trainxx, col = class_colors[trainyy], pch = 16)
### Question 2:
set.seed(456)
X1 = rnorm(n=100,0,1)
X2 = rnorm(n=100,0,1)
X3 = rnorm(n=100,0,1)
X4 = rnorm(n=100,0,1)
X5 = rnorm(n=100,0,1)
error = rnorm(n=100,0,0.5)
Y =2+3*X1+X2+2*X3+error
data = cbind(X1,X2,X3,X4,X5,Y)
sample <- sample(c(TRUE,FALSE), nrow(data),replace=TRUE, prob=c(0.7,0.3))
train_dataset  <- data[sample, ]
test_dataset  <- data[!sample, ]
### Forward Selection
### we set alpha = 0.05
alpha = 0.05
### when p = 1
Rss_list = vector(mode="list",length=0)
y=train_dataset[,6]
x=train_dataset[,1]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,1]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,2]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,2]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,3]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,3]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,4]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,4]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,5]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,5]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
Rss_list=unlist(Rss_list)
R1_1 = mean(Rss_list)
n = which.min(Rss_list)
print(paste("First Chosen One is X()",n))
### When p = 2
Rss_list = vector(mode="list",length=0)
y=train_dataset[,6]
x=train_dataset[,c(1,2)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,2)]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,c(1,3)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,3)]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,c(1,4)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,4)]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,c(1,5)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,5)]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
Rss_list=unlist(Rss_list)
n = which.min(Rss_list)
R2_2 = mean(Rss_list)
print("Second model in p=2 is chosen, so It is now X1~X3")
### When p = 3
Rss_list = vector(mode="list",length=0)
y=train_dataset[,6]
x=train_dataset[,c(1,2,3)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,2,3)]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,c(1,3,4)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,3,4)]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
y=train_dataset[,6]
x=train_dataset[,c(1,3,5)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,3,5)]))
RSS = sum((predict_y-test_dataset[,6])^2)
Rss_list= append(Rss_list,RSS)
Rss_list=unlist(Rss_list)
n = which.min(Rss_list)
R3_3 = mean(Rss_list)
print("Third model in p=3 is chosen, however model 2nd and 3rd all have p value very large, so we choose first model.")
print("This is given that critical value alpha = 0.05 > p-value of 2nd and 3rd model.")
RSS = c(R1_1,R2_2,R3_3)
index = c(1,2,3)
plot(index,RSS,type = "b",pch=18)
### It can be inferred from the graph that there is a elbow in p = 3, hence there is no need to further more add variable
### And current R^2 is 0.987
### Backward Selection:
### we set alpha = 0.05
alpha = 0.05
y=train_dataset[,6]
x=train_dataset[,1:5]
fit = lm(y~x)
summary(fit)
print("We can see that X5 has largest p value, so we discard X5 now.")
y=train_dataset[,6]
x=train_dataset[,1:4]
fit = lm(y~x)
summary(fit)
print("We can see that X5 has largest p value, so we discard X4 now.")
y=train_dataset[,6]
x=train_dataset[,1:3]
fit = lm(y~x)
summary(fit)
print("Now that P-value for every variable is 2e-16<0.05=alpha now, So Y~X1+X2+X3 is the final model.")
### Best Model Selection
### p = 1
Rsquare <- function(pred,test){
SSr = sum((pred-test)^2)
SSt = sum((test-mean(test))^2)
R2 = SSr/SSt
return (R2)
}
Rz = vector(mode="list",length=0)
y=train_dataset[,6]
x=train_dataset[,1]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,1]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
y=train_dataset[,6]
x=train_dataset[,2]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,2]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
y=train_dataset[,6]
x=train_dataset[,3]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,3]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
y=train_dataset[,6]
x=train_dataset[,4]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,4]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
y=train_dataset[,6]
x=train_dataset[,5]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,5]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
### p = 2
y=train_dataset[,6]
x=train_dataset[,c(1,2)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,2)]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
y=train_dataset[,6]
x=train_dataset[,c(1,3)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,3)]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
y=train_dataset[,6]
x=train_dataset[,c(1,4)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,4)]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
y=train_dataset[,6]
x=train_dataset[,c(1,5)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,5)]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
y=train_dataset[,6]
x=train_dataset[,c(1,2,3)]
fit = lm(y~x)
summary(fit)
predict_y = predict.lm(fit,data.frame(x=test_dataset[,c(1,2,3)]))
R = Rsquare(predict_y,test_dataset[,6])
Rz = append(Rz,summary(fit)$r.squared)
View(Rz)
